{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets huggingface_hub fsspec==2024.10.0 -qqq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T00:42:44.954465Z","iopub.execute_input":"2025-02-05T00:42:44.954796Z","iopub.status.idle":"2025-02-05T00:42:53.002244Z","shell.execute_reply.started":"2025-02-05T00:42:44.954770Z","shell.execute_reply":"2025-02-05T00:42:53.001322Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.6/179.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\ns3fs 2024.9.0 requires fsspec==2024.9.0.*, but you have fsspec 2024.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import os\nos.environ[\"OPENAI_API_KEY\"] = ''\nos.environ[\"HF_TOKEN\"] = ''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T00:42:53.003619Z","iopub.execute_input":"2025-02-05T00:42:53.003979Z","iopub.status.idle":"2025-02-05T00:42:53.008249Z","shell.execute_reply.started":"2025-02-05T00:42:53.003934Z","shell.execute_reply":"2025-02-05T00:42:53.007348Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Hugging face Tokenizer\n- decode\n- encode\n- convert","metadata":{}},{"cell_type":"code","source":"# https://huggingface.co/klue/roberta-base\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\nroberta_tokenizer = tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\nroberta_tokenizer\n# model = AutoModelForMaskedLM.from_pretrained(\"klue/roberta-base\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T00:42:53.010206Z","iopub.execute_input":"2025-02-05T00:42:53.010466Z","iopub.status.idle":"2025-02-05T00:42:53.126415Z","shell.execute_reply.started":"2025-02-05T00:42:53.010443Z","shell.execute_reply":"2025-02-05T00:42:53.125459Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"BertTokenizerFast(name_or_path='klue/roberta-base', vocab_size=32000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t0: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t3: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\n)"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"### 하나에 문장","metadata":{}},{"cell_type":"code","source":"text = '토크나이저는 텍스트를 사전에 있는 기준으로 토큰 단위로 나눈다'\n# tokenized_text = roberta_tokenizer(text) # encode\ntokenized_text = roberta_tokenizer.encode(text) # encode\n# tokenized_text = roberta_tokenizer.encode(text, add_special_tokens = False) # encode # False는 0과 2 안들어감\ntokenized_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T01:04:28.302933Z","iopub.execute_input":"2025-02-05T01:04:28.303221Z","iopub.status.idle":"2025-02-05T01:04:28.309297Z","shell.execute_reply.started":"2025-02-05T01:04:28.303198Z","shell.execute_reply":"2025-02-05T01:04:28.308474Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"[0,\n 9157,\n 7461,\n 2190,\n 2259,\n 8509,\n 2138,\n 4858,\n 2170,\n 1513,\n 2259,\n 3872,\n 6233,\n 1793,\n 2855,\n 5385,\n 2200,\n 20950,\n 2]"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"roberta_tokenizer.convert_ids_to_tokens(tokenized_text['input_ids'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T00:42:53.134607Z","iopub.execute_input":"2025-02-05T00:42:53.134940Z","iopub.status.idle":"2025-02-05T00:42:53.170224Z","shell.execute_reply.started":"2025-02-05T00:42:53.134907Z","shell.execute_reply":"2025-02-05T00:42:53.168555Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-d20ec2cd6c18>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mroberta_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"],"ename":"TypeError","evalue":"list indices must be integers or slices, not str","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"roberta_tokenizer.decode(tokenized_text['input_ids'], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T00:42:53.170657Z","iopub.status.idle":"2025-02-05T00:42:53.170920Z","shell.execute_reply":"2025-02-05T00:42:53.170816Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 여러 문장 변경","metadata":{}},{"cell_type":"code","source":"texts = ['첫 번째 문장 넣기', '두 번째 문장 작성 넣기']\n\ntokenized_texts = roberta_tokenizer(texts)\ntokenized_texts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T00:42:53.171757Z","iopub.status.idle":"2025-02-05T00:42:53.172146Z","shell.execute_reply":"2025-02-05T00:42:53.171981Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"roberta_tokenizer.batch_decode(tokenized_texts['input_ids'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T00:42:53.172891Z","iopub.status.idle":"2025-02-05T00:42:53.173270Z","shell.execute_reply":"2025-02-05T00:42:53.173104Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 모델별 별도 토큰이 필요한 이유","metadata":{}},{"cell_type":"code","source":"texts_kr = ['첫 번째 문장 넣기', '두 번째 문장 작성 넣기']\ntexts_eng = ['input first sentencs', 'input second sentencs']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T00:45:53.601358Z","iopub.execute_input":"2025-02-05T00:45:53.601638Z","iopub.status.idle":"2025-02-05T00:45:53.605223Z","shell.execute_reply.started":"2025-02-05T00:45:53.601607Z","shell.execute_reply":"2025-02-05T00:45:53.604404Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"bert_kr_tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\nprint(bert_kr_tokenizer(texts_kr))\nprint(bert_kr_tokenizer(texts_eng))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T00:45:54.258198Z","iopub.execute_input":"2025-02-05T00:45:54.258532Z","iopub.status.idle":"2025-02-05T00:45:54.371771Z","shell.execute_reply.started":"2025-02-05T00:45:54.258504Z","shell.execute_reply":"2025-02-05T00:45:54.370963Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': [[2, 1656, 1141, 3135, 6265, 751, 2015, 3], [2, 864, 1141, 3135, 6265, 5159, 751, 2015, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n{'input_ids': [[2, 5011, 2006, 6449, 73, 24507, 17219, 30062, 15698, 2041, 3], [2, 5011, 2006, 6449, 29318, 10613, 17219, 30062, 15698, 2041, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"roberta_kr_tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\nroberta_kr_tokenizer(texts_kr), roberta_kr_tokenizer(texts_eng)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T00:45:55.170707Z","iopub.execute_input":"2025-02-05T00:45:55.171046Z","iopub.status.idle":"2025-02-05T00:45:55.268420Z","shell.execute_reply.started":"2025-02-05T00:45:55.171019Z","shell.execute_reply":"2025-02-05T00:45:55.267516Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"({'input_ids': [[0, 1656, 1141, 3135, 6265, 751, 2015, 2], [0, 864, 1141, 3135, 6265, 5159, 751, 2015, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n {'input_ids': [[0, 5011, 2006, 6449, 73, 24507, 17219, 30062, 15698, 2041, 2], [0, 5011, 2006, 6449, 29318, 10613, 17219, 30062, 15698, 2041, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]})"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"roberta_eng_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\nprint(bert_tokenizer(texts_kr))\nprint(bert_tokenizer(texts_eng))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T00:45:55.898818Z","iopub.execute_input":"2025-02-05T00:45:55.899147Z","iopub.status.idle":"2025-02-05T00:45:56.103947Z","shell.execute_reply.started":"2025-02-05T00:45:55.899117Z","shell.execute_reply":"2025-02-05T00:45:56.102961Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': [[2, 1656, 1141, 3135, 6265, 751, 2015, 3], [2, 864, 1141, 3135, 6265, 5159, 751, 2015, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n{'input_ids': [[2, 5011, 2006, 6449, 73, 24507, 17219, 30062, 15698, 2041, 3], [2, 5011, 2006, 6449, 29318, 10613, 17219, 30062, 15698, 2041, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}