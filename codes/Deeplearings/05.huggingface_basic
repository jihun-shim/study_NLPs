{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers datasets huggingface_hub fsspec==2024.10.0 -qqq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T06:49:31.820258Z","iopub.execute_input":"2025-02-04T06:49:31.820456Z","iopub.status.idle":"2025-02-04T06:49:39.445613Z","shell.execute_reply.started":"2025-02-04T06:49:31.820438Z","shell.execute_reply":"2025-02-04T06:49:39.444805Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.6/179.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\ns3fs 2024.9.0 requires fsspec==2024.9.0.*, but you have fsspec 2024.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Hugging face 사용 이유\n- 여러 회사들이 개발한 모델이나 제공하는 데이터셋을 편리하게 사용하는 인터페이스 제공","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModel, AutoTokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T06:53:43.000051Z","iopub.execute_input":"2025-02-04T06:53:43.000356Z","iopub.status.idle":"2025-02-04T06:53:50.440375Z","shell.execute_reply.started":"2025-02-04T06:53:43.000329Z","shell.execute_reply":"2025-02-04T06:53:50.439474Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Bert 모델 활용 준비\n- https://huggingface.co/google-bert/bert-base-uncased","metadata":{}},{"cell_type":"code","source":"bert_model = AutoModel.from_pretrained('google-bert/bert-base-uncased')\nbert_tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-uncased')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T06:59:13.981012Z","iopub.execute_input":"2025-02-04T06:59:13.981316Z","iopub.status.idle":"2025-02-04T06:59:14.312251Z","shell.execute_reply.started":"2025-02-04T06:59:13.981293Z","shell.execute_reply":"2025-02-04T06:59:14.311381Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"type(bert_model), type(bert_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T06:59:46.083147Z","iopub.execute_input":"2025-02-04T06:59:46.083468Z","iopub.status.idle":"2025-02-04T06:59:46.088188Z","shell.execute_reply.started":"2025-02-04T06:59:46.083439Z","shell.execute_reply":"2025-02-04T06:59:46.087532Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(transformers.models.bert.modeling_bert.BertModel,\n transformers.models.bert.tokenization_bert_fast.BertTokenizerFast)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"text = 'What is Huggingface Transformers ?'\nencoded_input = bert_tokenizer(text, return_tensors='pt') # 학습 완료된 형태에 벡터로 결과 리턴\nbert_out = bert_model(**encoded_input)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:03:57.957912Z","iopub.execute_input":"2025-02-04T07:03:57.958219Z","iopub.status.idle":"2025-02-04T07:03:58.134640Z","shell.execute_reply.started":"2025-02-04T07:03:57.958196Z","shell.execute_reply":"2025-02-04T07:03:58.133893Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# type(encoded_input), encoded_input\ntype(bert_out), bert_out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:04:55.998174Z","iopub.execute_input":"2025-02-04T07:04:55.998494Z","iopub.status.idle":"2025-02-04T07:04:56.031361Z","shell.execute_reply.started":"2025-02-04T07:04:55.998468Z","shell.execute_reply":"2025-02-04T07:04:56.030583Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions,\n BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.3009,  0.0158,  0.0698,  ..., -0.3406,  0.5976,  0.5820],\n          [-0.1109,  0.0754, -0.1906,  ...,  0.2970,  0.4278, -0.0391],\n          [-0.5813, -0.0042,  0.4034,  ..., -0.2549,  0.2216,  0.8121],\n          ...,\n          [ 0.9971,  0.3301, -0.0688,  ..., -0.4873,  0.0168, -0.0345],\n          [-0.2394, -0.0573, -0.5885,  ..., -0.0415,  0.3123, -0.0288],\n          [ 0.7884,  0.4039,  0.0217,  ...,  0.3869, -0.4785, -0.4116]]],\n        grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-8.7247e-01, -3.1464e-01, -5.2733e-01,  7.3685e-01,  1.8460e-01,\n          -1.8295e-01,  8.9985e-01,  2.9762e-01, -3.9314e-01, -9.9994e-01,\n           7.1273e-02,  7.2618e-01,  9.7232e-01,  3.2968e-01,  9.1369e-01,\n          -7.5043e-01, -3.5754e-01, -5.6530e-01,  2.8968e-01, -6.4040e-01,\n           6.0199e-01,  9.9856e-01,  3.8675e-01,  2.9078e-01,  4.0431e-01,\n           7.7010e-01, -7.6515e-01,  9.1830e-01,  9.4834e-01,  7.2557e-01,\n          -7.3735e-01,  1.9867e-01, -9.8172e-01, -1.8334e-01, -4.9430e-01,\n          -9.8059e-01,  2.6051e-01, -7.4618e-01,  4.8119e-02,  5.9711e-02,\n          -8.1904e-01,  2.1572e-01,  9.9974e-01, -6.2062e-01,  1.3179e-02,\n          -3.7181e-01, -1.0000e+00,  2.5721e-01, -8.5921e-01,  3.5142e-01,\n           4.2221e-01,  2.7741e-02,  1.5075e-01,  3.6331e-01,  4.4037e-01,\n           2.4009e-02, -1.7561e-01,  6.5655e-02, -2.2943e-01, -5.1775e-01,\n          -6.1054e-01,  4.0539e-01, -4.9568e-01, -8.7416e-01,  5.6503e-01,\n           5.4685e-01, -9.5151e-02, -2.8588e-01, -6.3585e-02, -5.9149e-02,\n           8.8904e-01,  1.0488e-01,  2.8849e-01, -8.3510e-01,  1.3450e-01,\n           2.2096e-01, -6.5445e-01,  1.0000e+00, -5.5778e-01, -9.5749e-01,\n           4.4432e-01,  5.2114e-01,  5.8759e-01,  2.3123e-02,  3.6001e-01,\n          -1.0000e+00,  2.6260e-01, -6.2052e-02, -9.8500e-01,  2.5122e-01,\n           4.3635e-01, -1.6588e-01,  3.9132e-01,  5.3988e-01, -5.7578e-01,\n          -1.7328e-01, -8.3041e-02, -3.6534e-01, -1.3268e-01, -1.2891e-01,\n           2.1034e-02, -2.0756e-01, -2.4547e-01, -3.3774e-01,  2.4423e-01,\n          -4.7804e-01, -4.5389e-01,  3.9510e-01,  7.4778e-02,  6.1011e-01,\n           5.0728e-01, -3.0130e-01,  3.7723e-01, -9.3758e-01,  4.9906e-01,\n          -3.2958e-01, -9.8185e-01, -6.1150e-01, -9.7898e-01,  6.7381e-01,\n          -6.0384e-02, -2.6210e-01,  9.3328e-01,  1.9747e-01,  3.5069e-01,\n          -7.0665e-02, -3.4704e-01, -1.0000e+00, -2.7822e-01, -3.7445e-01,\n          -1.7645e-01, -2.1203e-01, -9.6432e-01, -9.4622e-01,  5.7054e-01,\n           9.4001e-01,  1.8941e-01,  9.9923e-01, -1.5256e-01,  9.1510e-01,\n           7.8501e-02, -5.6503e-01, -7.2293e-02, -4.5555e-01,  5.5808e-01,\n           1.8401e-01, -6.0199e-01,  2.3481e-01, -3.3688e-02, -2.7570e-01,\n          -5.3239e-01, -1.7641e-01, -4.0169e-01, -9.2079e-01, -3.1998e-01,\n           9.3809e-01, -7.1151e-02, -6.9783e-01,  6.7255e-02, -2.0274e-01,\n          -3.7808e-01,  8.4576e-01,  5.0007e-01,  3.6423e-01, -4.1740e-01,\n           3.6109e-01,  1.4876e-01,  3.5860e-01, -8.4838e-01,  2.6076e-01,\n           4.1682e-01, -3.1267e-01, -5.2371e-01, -9.6102e-01, -2.9391e-01,\n           5.4447e-01,  9.8303e-01,  7.4894e-01,  2.8395e-01,  1.4951e-01,\n          -2.6745e-01,  1.8008e-01, -9.3940e-01,  9.6656e-01, -1.0958e-01,\n           1.8235e-01,  1.6497e-01,  2.4865e-01, -8.2815e-01,  7.0330e-04,\n           8.1064e-01, -1.2263e-02, -8.1630e-01, -4.3830e-02, -4.7633e-01,\n          -3.8495e-01, -3.8118e-01,  4.5080e-01, -1.9782e-01, -3.4960e-01,\n           1.1252e-01,  8.9026e-01,  9.7578e-01,  6.9775e-01, -1.2651e-01,\n           5.7326e-01, -8.3355e-01, -4.0410e-01,  5.5684e-02,  1.7854e-01,\n           8.9321e-02,  9.8696e-01, -3.9308e-01, -7.7631e-02, -9.1122e-01,\n          -9.7244e-01,  5.5037e-03, -8.4574e-01, -2.4717e-02, -6.3462e-01,\n           4.9246e-01,  5.5000e-01,  1.1418e-01,  3.0862e-01, -9.6484e-01,\n          -7.7138e-01,  3.5477e-01, -2.1973e-01,  4.2417e-01, -2.4453e-01,\n           6.6965e-01,  6.5433e-01, -6.3527e-01,  8.0241e-01,  9.1417e-01,\n          -4.4787e-01, -6.7925e-01,  8.3256e-01, -2.6945e-01,  8.7207e-01,\n          -5.2281e-01,  9.8420e-01,  4.2696e-01,  4.8134e-01, -8.7702e-01,\n          -5.7352e-01, -8.7672e-01, -3.6707e-01,  7.0803e-02, -1.4201e-01,\n           6.3289e-01,  6.3346e-01,  2.9797e-01,  4.1055e-01, -6.0411e-01,\n           9.9386e-01, -7.6779e-01, -9.4760e-01,  2.8060e-01,  1.7031e-02,\n          -9.8417e-01,  3.4940e-01,  2.5224e-01, -4.0932e-01, -3.8472e-01,\n          -6.1061e-01, -9.4425e-01,  8.4996e-01,  1.8030e-01,  9.8473e-01,\n          -8.3624e-02, -8.9589e-01, -4.1394e-01, -8.9448e-01, -2.8790e-01,\n          -1.7622e-01,  1.2934e-01, -1.4377e-01, -9.4701e-01,  4.8915e-01,\n           5.5083e-01,  3.5019e-01, -5.7310e-01,  9.9580e-01,  9.9998e-01,\n           9.5247e-01,  8.9147e-01,  8.6635e-01, -9.9394e-01, -3.3778e-01,\n           9.9998e-01, -9.1078e-01, -1.0000e+00, -9.3294e-01, -5.2658e-01,\n           3.7828e-01, -1.0000e+00, -3.1776e-01,  7.3381e-02, -8.6886e-01,\n           3.6298e-01,  9.6532e-01,  9.8247e-01, -1.0000e+00,  8.0954e-01,\n           9.0122e-01, -6.6414e-01,  6.7349e-01, -3.1281e-01,  9.6308e-01,\n           5.0662e-01,  4.2693e-01, -2.2366e-01,  3.7596e-01, -8.1644e-01,\n          -8.3308e-01, -1.3696e-01, -4.6714e-01,  9.4581e-01,  1.8325e-01,\n          -7.8852e-01, -8.7354e-01, -5.8154e-02, -6.9012e-02, -3.4173e-01,\n          -9.4760e-01, -1.7819e-01, -1.3929e-01,  6.8355e-01,  4.5168e-02,\n           2.6814e-01, -6.8768e-01,  2.1210e-01, -2.3397e-01,  3.4735e-01,\n           6.8611e-01, -9.2963e-01, -5.2597e-01,  3.0077e-01, -3.7891e-01,\n          -4.7398e-01, -9.5464e-01,  9.4878e-01, -3.2371e-01,  3.0548e-01,\n           1.0000e+00, -1.6540e-01, -7.9525e-01,  5.4072e-01,  1.6751e-01,\n          -1.2475e-01,  1.0000e+00,  6.4986e-01, -9.5677e-01, -6.0413e-01,\n           3.7020e-01, -4.8581e-01, -4.7777e-01,  9.9866e-01, -2.5729e-01,\n          -2.4091e-01,  1.1121e-01,  9.6287e-01, -9.8189e-01,  9.3767e-01,\n          -8.7587e-01, -9.4305e-01,  9.3710e-01,  9.0091e-01, -3.8187e-01,\n          -6.1976e-01,  1.4977e-01, -5.8692e-01,  2.6118e-01, -9.5406e-01,\n           6.2279e-01,  4.8689e-01, -8.8539e-02,  8.5752e-01, -8.3492e-01,\n          -5.3720e-01,  2.5680e-01, -4.1266e-01,  2.2846e-01,  5.8122e-01,\n           4.3429e-01, -2.4578e-01,  1.2280e-01, -2.9919e-01, -3.1362e-01,\n          -9.6913e-01,  6.2338e-02,  1.0000e+00,  1.3629e-02, -1.2817e-01,\n          -3.8745e-01,  1.8795e-02, -1.8447e-01,  4.2568e-01,  5.1443e-01,\n          -2.2532e-01, -8.0441e-01,  1.4079e-01, -9.4743e-01, -9.8253e-01,\n           7.6432e-01,  1.4451e-01, -3.2291e-01,  9.9988e-01,  3.1823e-01,\n           1.5380e-01,  1.3672e-01,  7.6152e-01,  1.0233e-01,  5.8834e-01,\n           5.5959e-01,  9.6941e-01, -2.7468e-01,  6.1814e-01,  8.2141e-01,\n          -5.6554e-01, -2.7680e-01, -6.2172e-01, -8.9433e-02, -9.0074e-01,\n           1.2009e-01, -9.2751e-01,  9.4329e-01,  4.9251e-01,  2.7484e-01,\n           2.1843e-01,  6.6064e-02,  1.0000e+00, -2.2593e-01,  6.4518e-01,\n          -5.1134e-01,  8.2547e-01, -9.8957e-01, -7.8994e-01, -3.4982e-01,\n          -8.3618e-02, -3.6136e-01, -2.2800e-01,  2.0661e-01, -9.5846e-01,\n           3.5442e-01,  9.7108e-02, -9.7417e-01, -9.8264e-01,  2.9154e-01,\n           8.3737e-01,  6.6633e-02, -8.0501e-01, -6.1920e-01, -5.9091e-01,\n           1.7083e-01, -1.7838e-01, -9.0619e-01,  2.4375e-01, -1.7610e-01,\n           4.1765e-01, -2.2214e-01,  5.1794e-01,  5.8822e-01,  6.4995e-01,\n           1.3125e-01, -9.7188e-02, -8.3228e-02, -8.4713e-01,  8.4501e-01,\n          -7.9905e-01, -4.0446e-01, -2.0988e-01,  1.0000e+00, -5.1438e-01,\n           6.3233e-01,  7.9540e-01,  6.0073e-01, -1.3826e-01,  2.1753e-01,\n           6.5091e-01,  1.4618e-01, -6.2026e-01, -4.0101e-01, -7.0176e-01,\n          -3.3691e-01,  6.5601e-01, -8.1213e-02,  4.6828e-01,  6.5125e-01,\n           4.2083e-01,  1.4356e-01, -3.9186e-02, -4.4730e-02,  9.9797e-01,\n          -1.2616e-01,  5.6323e-02, -5.0923e-01,  7.9207e-03, -3.3530e-01,\n          -4.9021e-01,  1.0000e+00,  2.7262e-01, -1.3420e-01, -9.8275e-01,\n          -3.7159e-01, -8.9745e-01,  9.9994e-01,  8.0115e-01, -7.2103e-01,\n           6.0988e-01,  2.2475e-01, -1.7136e-01,  7.3667e-01, -1.3005e-01,\n          -2.5390e-01,  2.7347e-01,  9.6592e-02,  9.2964e-01, -4.7674e-01,\n          -9.5305e-01, -6.8485e-01,  3.5561e-01, -9.3918e-01,  9.9616e-01,\n          -4.4199e-01, -1.8582e-01, -4.1399e-01,  4.5030e-01,  6.3904e-01,\n          -1.7178e-01, -9.7449e-01, -5.1295e-02,  1.6738e-01,  9.3006e-01,\n           1.3585e-01, -5.9081e-01, -9.1515e-01,  1.1425e-01,  4.2950e-01,\n          -5.2345e-01, -8.7475e-01,  9.4331e-01, -9.7478e-01,  3.9603e-01,\n           1.0000e+00,  3.9404e-01, -6.0146e-01,  2.9373e-02, -5.1601e-01,\n           2.4861e-01,  2.4923e-01,  6.7270e-01, -9.2070e-01, -2.8287e-01,\n          -7.6749e-02,  1.7067e-01, -8.0915e-02,  1.2053e-01,  5.9641e-01,\n           1.7944e-01, -5.4068e-01, -5.6923e-01, -2.5088e-02,  4.6722e-01,\n           8.3220e-01, -3.5286e-01, -1.6219e-01,  9.4050e-02, -9.4970e-02,\n          -8.7677e-01, -2.7396e-01, -2.1793e-01, -9.9938e-01,  7.5556e-01,\n          -1.0000e+00, -1.7111e-01, -1.8512e-01, -2.3656e-01,  7.9198e-01,\n           2.8102e-01,  2.7825e-01, -6.7673e-01, -5.1071e-01,  6.6396e-01,\n           6.7959e-01, -1.5748e-01,  1.9009e-01, -6.7916e-01,  1.1147e-01,\n          -4.1102e-03, -7.4860e-03, -1.5136e-01,  8.2224e-01, -1.6503e-01,\n           1.0000e+00,  1.4966e-01, -6.3775e-01, -9.6420e-01,  2.2951e-01,\n          -2.5219e-01,  1.0000e+00, -9.1262e-01, -9.3021e-01,  2.5156e-01,\n          -6.5498e-01, -8.3972e-01,  2.0794e-01,  1.4597e-01, -5.7221e-01,\n          -5.5899e-01,  9.3297e-01,  9.1276e-01, -6.3061e-01,  3.3351e-01,\n          -3.3155e-01, -3.4310e-01,  5.5119e-02,  1.7479e-01,  9.7497e-01,\n           1.8068e-01,  8.6350e-01,  4.1295e-01,  6.8414e-02,  9.4591e-01,\n           2.3672e-01,  4.7458e-01,  5.9379e-02,  1.0000e+00,  2.6528e-01,\n          -8.8648e-01,  3.4260e-01, -9.7573e-01, -1.5098e-01, -9.4319e-01,\n           2.2633e-01,  2.1388e-01,  8.7123e-01, -9.6188e-02,  9.3925e-01,\n           2.4527e-02, -9.8376e-02, -1.0091e-01,  1.4681e-01,  3.8430e-01,\n          -8.7014e-01, -9.7812e-01, -9.7528e-01,  3.6960e-01, -3.8164e-01,\n          -4.7706e-02,  1.4460e-01,  1.0588e-01,  3.0843e-01,  4.1187e-01,\n          -1.0000e+00,  9.1188e-01,  3.6028e-01,  7.0124e-01,  9.3118e-01,\n           5.1771e-01,  2.7587e-01,  2.8873e-01, -9.7848e-01, -9.4348e-01,\n          -3.1992e-01, -2.0700e-01,  6.7060e-01,  6.4204e-01,  8.7050e-01,\n           3.0572e-01, -4.9177e-01, -2.6046e-01,  1.0768e-01, -5.5766e-01,\n          -9.8769e-01,  3.4500e-01, -1.2493e-01, -9.4611e-01,  9.4435e-01,\n          -2.2333e-01, -1.4135e-01,  1.9776e-01, -3.9046e-01,  9.3880e-01,\n           6.8928e-01,  3.8767e-01,  1.1025e-01,  5.1922e-01,  8.3273e-01,\n           9.4730e-01,  9.7341e-01, -5.7427e-01,  7.8560e-01, -6.8619e-02,\n           4.1192e-01,  6.9466e-01, -9.3774e-01,  2.3971e-01,  1.3474e-01,\n          -4.2770e-01,  1.6046e-01, -2.4939e-01, -9.7068e-01,  5.0688e-01,\n          -2.9860e-01,  5.1771e-01, -3.1591e-01,  1.0868e-01, -3.8842e-01,\n          -1.5632e-01, -6.4045e-01, -5.4585e-01,  6.6964e-01,  3.3962e-01,\n           8.5658e-01,  5.4788e-01, -1.0526e-01, -5.1888e-01, -1.6210e-01,\n          -5.4360e-01, -8.8313e-01,  8.7812e-01,  4.3411e-02, -2.1279e-01,\n           3.2289e-01, -1.8518e-01,  6.5461e-01, -6.4463e-02, -3.7191e-01,\n          -3.7232e-01, -7.5502e-01,  8.0540e-01, -1.7699e-01, -5.2501e-01,\n          -5.6918e-01,  3.7711e-01,  3.8391e-01,  9.9864e-01, -5.3484e-01,\n          -5.4315e-01, -7.2384e-02, -2.4831e-01,  4.2074e-01, -2.2461e-01,\n          -1.0000e+00,  2.4747e-01,  7.5573e-02,  3.7574e-01, -7.1562e-02,\n           3.3389e-01, -1.6045e-01, -9.6561e-01, -1.6314e-01,  7.3813e-02,\n           3.5186e-01, -4.4838e-01, -2.2300e-01,  5.7876e-01,  6.1219e-01,\n           7.0052e-01,  8.2005e-01,  1.1350e-01,  2.0501e-01,  6.4786e-01,\n          -2.2237e-01, -6.3388e-01,  8.9791e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None))"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"## 모델 불러오기\n- 모델 : 바디와 헤드로 구분","metadata":{}},{"cell_type":"code","source":"# 바디만 있는 모델\n# https://huggingface.co/klue/roberta-base\nroberta_model = AutoModel.from_pretrained('klue/roberta-base')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:41:47.772789Z","iopub.execute_input":"2025-02-04T07:41:47.773095Z","iopub.status.idle":"2025-02-04T07:41:47.873883Z","shell.execute_reply.started":"2025-02-04T07:41:47.773071Z","shell.execute_reply":"2025-02-04T07:41:47.873042Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"roberta_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:50:41.151114Z","iopub.execute_input":"2025-02-04T07:50:41.151410Z","iopub.status.idle":"2025-02-04T07:50:41.157454Z","shell.execute_reply.started":"2025-02-04T07:50:41.151389Z","shell.execute_reply":"2025-02-04T07:50:41.156673Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"RobertaModel(\n  (embeddings): RobertaEmbeddings(\n    (word_embeddings): Embedding(32000, 768, padding_idx=1)\n    (position_embeddings): Embedding(514, 768, padding_idx=1)\n    (token_type_embeddings): Embedding(1, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): RobertaEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSdpaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): RobertaPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# 바디와 헤드가 포함된 모델\n# https://huggingface.co/SamLowe/roberta-base-go_emotions\nfrom transformers import AutoModelForSequenceClassification\nroberta_go_emotions_model = AutoModelForSequenceClassification.from_pretrained(\"SamLowe/roberta-base-go_emotions\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:49:08.272248Z","iopub.execute_input":"2025-02-04T07:49:08.272549Z","iopub.status.idle":"2025-02-04T07:49:08.405156Z","shell.execute_reply.started":"2025-02-04T07:49:08.272527Z","shell.execute_reply":"2025-02-04T07:49:08.404516Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"roberta_go_emotions_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:50:54.662808Z","iopub.execute_input":"2025-02-04T07:50:54.663093Z","iopub.status.idle":"2025-02-04T07:50:54.669139Z","shell.execute_reply.started":"2025-02-04T07:50:54.663073Z","shell.execute_reply":"2025-02-04T07:50:54.668478Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=28, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"# 헤드에 없는 모델을 목적에 의해 불러오기\nroberta_classification_model = AutoModelForSequenceClassification.from_pretrained('klue/roberta-base')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:53:33.195492Z","iopub.execute_input":"2025-02-04T07:53:33.195803Z","iopub.status.idle":"2025-02-04T07:53:33.462456Z","shell.execute_reply.started":"2025-02-04T07:53:33.195779Z","shell.execute_reply":"2025-02-04T07:53:33.461652Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"roberta_classification_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T07:53:29.657822Z","iopub.execute_input":"2025-02-04T07:53:29.658115Z","iopub.status.idle":"2025-02-04T07:53:29.664055Z","shell.execute_reply.started":"2025-02-04T07:53:29.658093Z","shell.execute_reply":"2025-02-04T07:53:29.663277Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(32000, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}